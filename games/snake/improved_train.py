#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
games/snake/improved_train.py - ì„±ëŠ¥ ê°œì„ ëœ Snake AI í›ˆë ¨

ê¸°ì¡´ train.pyì˜ ì„±ëŠ¥ ë¬¸ì œë¥¼ í•´ê²°í•œ ê°œì„ ëœ ë²„ì „
í‰ê·  ì ìˆ˜ 0.7ì  â†’ 10+ì ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ ëª©í‘œ

Author: ì¡°ì£¼ì€ Lily
Created: 2025-08-05
"""

import os
import sys
import numpy as np
import pickle
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple
import time

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“ˆë“¤ ì„í¬íŠ¸ë¥¼ ìœ„í•œ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(project_root)

from games.snake.game import SnakeGameAI
from games.snake.agent import QLearningAgent

# ë‹¤í¬ëª¨ë“œ ì¹œí™”ì  ì»¬ëŸ¬ ì½”ë“œ
class Colors:
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    WHITE = '\033[97m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

class ImprovedQLearningAgent(QLearningAgent):
    """ì„±ëŠ¥ ê°œì„ ëœ Q-Learning ì—ì´ì „íŠ¸"""
    
    def __init__(self, 
                 learning_rate=0.7,        # ê¸°ì¡´ 0.3 â†’ 0.7ë¡œ ëŒ€í­ ì¦ê°€
                 discount_factor=0.99,     # ê¸°ì¡´ 0.95 â†’ 0.99ë¡œ ì¦ê°€  
                 epsilon=0.9,              # ê¸°ì¡´ 0.8 â†’ 0.9ë¡œ ì¦ê°€ (ì´ˆê¸° íƒí—˜ ê·¹ëŒ€í™”)
                 epsilon_decay=0.9995,     # ì²œì²œíˆ ê°ì†Œ
                 epsilon_min=0.1):         # ìµœì†Œê°’ì„ ë†’ê²Œ ìœ ì§€
        
        # super().__init__() í˜¸ì¶œ ì „ì— ì†ì„±ì„ ë¨¼ì € ì„¤ì •í•©ë‹ˆë‹¤.
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.q_table = {}
        
        print(f"{Colors.GREEN}ğŸš€ ê°œì„ ëœ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”{Colors.RESET}")
        print(f"   í•™ìŠµë¥ : {self.learning_rate} (ê¸°ì¡´ ëŒ€ë¹„ +133%)")
        print(f"   í• ì¸ì¸ì: {self.discount_factor} (ê¸°ì¡´ ëŒ€ë¹„ +4%)")
        print(f"   ì´ˆê¸° íƒí—˜ìœ¨: {self.epsilon} (ê¸°ì¡´ ëŒ€ë¹„ +12.5%)")
        print(f"   ìµœì†Œ íƒí—˜ìœ¨: {self.epsilon_min} (ê¸°ì¡´ ëŒ€ë¹„ +1000%)")
        # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ __init__ì„ í˜¸ì¶œí•˜ì—¬ update_q_tableê³¼ ê°™ì€ ë©”ì„œë“œë¥¼ ìƒì†ë°›ìŠµë‹ˆë‹¤.
        super().__init__(learning_rate, discount_factor, epsilon, epsilon_decay, epsilon_min)
    
    def get_action(self, state):
        """ê°œì„ ëœ í–‰ë™ ì„ íƒ - ë” ì ê·¹ì ì¸ íƒí—˜"""
        state_key = str(state)
        
        # Q-í…Œì´ë¸” ì´ˆê¸°í™” (ë” ë„“ì€ ë²”ìœ„ì˜ ì´ˆê¸°ê°’)
        if state_key not in self.q_table:
            self.q_table[state_key] = np.random.uniform(-2, 2, 3)  # ê¸°ì¡´ [-1,1] â†’ [-2,2]
        
        # Epsilon-greedy with better exploration
        if np.random.random() < self.epsilon:
            # ì™„ì „ ëœë¤ì´ ì•„ë‹Œ ìŠ¤ë§ˆíŠ¸í•œ íƒí—˜
            if np.random.random() < 0.3:  # 30%ëŠ” ì™„ì „ ëœë¤
                return np.random.randint(0, 3)
            else:  # 70%ëŠ” Qê°’ì´ ë‚®ì€ í–‰ë™ ìš°ì„  (underexplored actions)
                q_values = self.q_table[state_key]
                # ê°€ì¥ ì ê²Œ ì‹œë„ëœ í–‰ë™ì— ì•½ê°„ì˜ ìš°ì„ ê¶Œ
                return np.argmin(q_values) if np.random.random() < 0.5 else np.random.randint(0, 3)
        else:
            return np.argmax(self.q_table[state_key])

def improved_reward_function(game, prev_score, curr_score, done, steps):
    """ëŒ€í­ ê°œì„ ëœ ë³´ìƒ í•¨ìˆ˜"""
    reward = 0
    
    # 1. ì ìˆ˜ ì¦ê°€ì‹œ í° ë³´ìƒ
    score_diff = curr_score - prev_score
    if score_diff > 0:
        reward += score_diff * 20  # ê¸°ì¡´ 10 â†’ 20ìœ¼ë¡œ ì¦ê°€
        print(f"ğŸ ë¨¹ì´ íšë“! ë³´ìƒ: +{score_diff * 20}")
    
    # 2. ë¨¹ì´ì™€ì˜ ê±°ë¦¬ ê¸°ë°˜ ë³´ìƒ (í•µì‹¬ ê°œì„ ì‚¬í•­)
    if hasattr(game, 'head') and hasattr(game, 'food'):
        head_x, head_y = game.head.x, game.head.y
        food_x, food_y = game.food.x, game.food.y
        
        # ë§¨í•˜íƒ„ ê±°ë¦¬ ê³„ì‚°
        distance = abs(head_x - food_x) + abs(head_y - food_y)
        max_distance = (game.w // 20) + (game.h // 20)  # ìµœëŒ€ ê°€ëŠ¥ ê±°ë¦¬ (ë¸”ë¡ ë‹¨ìœ„)
        
        # ê±°ë¦¬ì— ë°˜ë¹„ë¡€í•˜ëŠ” ë³´ìƒ (ê°€ê¹Œìš¸ìˆ˜ë¡ í° ë³´ìƒ)
        distance_reward = (max_distance - distance) / max_distance * 2
        reward += distance_reward
        
        # ë§¤ìš° ê°€ê¹Œìš°ë©´ ì¶”ê°€ ë³´ìƒ
        if distance <= 2:
            reward += 5
        elif distance <= 4:
            reward += 2
        elif distance <= 6:
            reward += 1
    
    # 3. ìƒì¡´ ë³´ìƒ (ìƒì¡´ ìì²´ì— ì˜ë¯¸ ë¶€ì—¬)
    reward += 0.2  # ê¸°ì¡´ 0.1 â†’ 0.2
    
    # 4. ì¶©ëŒì‹œ í° í˜ë„í‹°
    if done:
        reward = -50  # ê¸°ì¡´ -10 â†’ -50ìœ¼ë¡œ ì¦ê°€
        print(f"ğŸ’€ ì¶©ëŒ! í˜ë„í‹°: -50")
    
    # 5. ë„ˆë¬´ ì˜¤ë˜ ì‚´ì•„ìˆìœ¼ë©´ ì‘ì€ í˜ë„í‹° (ë¬´í•œ ë£¨í”„ ë°©ì§€)
    if steps > 500:
        reward -= 0.5
    
    return reward

def visualize_training_progress(scores, avg_scores, save_path="training_progress.png"):
    """í›ˆë ¨ ì§„í–‰ ìƒí™© ì‹œê°í™”"""
    plt.style.use('dark_background')  # ë‹¤í¬ëª¨ë“œ
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
    
    # ê°œë³„ ì ìˆ˜
    ax1.plot(scores, color='#64ffda', alpha=0.6, linewidth=1)
    ax1.set_title('ğŸ¯ ì—í”¼ì†Œë“œë³„ ì ìˆ˜', color='#e0e6ed', fontsize=14, fontweight='bold')
    ax1.set_xlabel('ì—í”¼ì†Œë“œ', color='#e0e6ed')
    ax1.set_ylabel('ì ìˆ˜', color='#e0e6ed')
    ax1.grid(True, alpha=0.3)
    
    # ì´ë™ í‰ê· 
    if avg_scores:
        ax2.plot(avg_scores, color='#bb86fc', linewidth=3)
        ax2.set_title('ğŸ“ˆ ì´ë™ í‰ê·  ì ìˆ˜ (100 ì—í”¼ì†Œë“œ)', color='#e0e6ed', fontsize=14, fontweight='bold')
        ax2.set_xlabel('ì—í”¼ì†Œë“œ (x100)', color='#e0e6ed') 
        ax2.set_ylabel('í‰ê·  ì ìˆ˜', color='#e0e6ed')
        ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, facecolor='#1a1a1a', dpi=150)
    plt.show()
    
    print(f"{Colors.GREEN}ğŸ“Š í›ˆë ¨ ê·¸ë˜í”„ ì €ì¥: {save_path}{Colors.RESET}")

def run_improved_training(episodes=3000, save_interval=500, model_name="snake_improved_agent"):
    """ê°œì„ ëœ í›ˆë ¨ ë©”ì¸ í•¨ìˆ˜"""
    
    print(f"{Colors.MAGENTA}{'='*70}{Colors.RESET}")
    print(f"{Colors.CYAN}ğŸ¯ ê°œì„ ëœ Snake AI í›ˆë ¨ ì‹œì‘{Colors.RESET}")
    print(f"{Colors.WHITE}ëª©í‘œ: í‰ê·  ì ìˆ˜ 0.7 â†’ 10+ ë‹¬ì„±{Colors.RESET}")
    print(f"{Colors.BLUE}ì—í”¼ì†Œë“œ: {episodes}, ì €ì¥ ê°„ê²©: {save_interval}{Colors.RESET}")
    print(f"{Colors.MAGENTA}{'='*70}{Colors.RESET}")
    
    # í™˜ê²½ ë° ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    game = SnakeGameAI()
    agent = ImprovedQLearningAgent()
    
    # í›ˆë ¨ ê¸°ë¡
    scores = []
    avg_scores = []
    best_score = 0
    best_avg_score = 0
    
    start_time = time.time()
    
    for episode in range(episodes):
        # ì—í”¼ì†Œë“œ ì´ˆê¸°í™”
        state = game.reset()
        prev_score = 0
        total_reward = 0
        steps = 0
        
        while True:
            # í–‰ë™ ì„ íƒ ë° ì‹¤í–‰
            action = agent.get_action(state)
            
            # ê²Œì„ ìŠ¤í… (ì‹¤ì œ ë°˜í™˜ê°’ì— ë§ì¶¤)
            play_result = game.play_step(action)
            curr_score = game.score if hasattr(game, 'score') else 0
            
            # ê²Œì„ ì¢…ë£Œ ì¡°ê±´
            done = game.is_collision() if hasattr(game, 'is_collision') else False
            
            # ê°œì„ ëœ ë³´ìƒ ê³„ì‚°
            reward = improved_reward_function(game, prev_score, curr_score, done, steps)
            total_reward += reward
            
            # ë‹¤ìŒ ìƒíƒœ
            next_state = game.get_state()
            
            # Q-ëŸ¬ë‹ ì—…ë°ì´íŠ¸
            agent.update_q_table(state, action, reward, next_state, done)
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state = next_state
            prev_score = curr_score
            steps += 1
            
            # ì¢…ë£Œ ì¡°ê±´
            if done or steps > 1000:  # ìµœëŒ€ ìŠ¤í… ì¦ê°€
                break
        
        # ì—í”¼ì†Œë“œ ê²°ê³¼ ê¸°ë¡
        final_score = curr_score
        scores.append(final_score)
        
        # ìµœê³  ì ìˆ˜ ì—…ë°ì´íŠ¸
        if final_score > best_score:
            best_score = final_score
        
        # ì´ë™ í‰ê·  ê³„ì‚°
        if len(scores) >= 100:
            avg_score = np.mean(scores[-100:])
            avg_scores.append(avg_score)
            
            if avg_score > best_avg_score:
                best_avg_score = avg_score
        
        # Epsilon ê°ì†Œ
        agent.decay_epsilon()
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if episode % 100 == 0:
            elapsed_time = time.time() - start_time
            recent_avg = np.mean(scores[-100:]) if len(scores) >= 100 else np.mean(scores)
            
            print(f"{Colors.YELLOW}ì—í”¼ì†Œë“œ {episode:4d}{Colors.RESET} | "
                  f"ì ìˆ˜: {final_score:2d} | "
                  f"í‰ê· : {recent_avg:5.2f} | "
                  f"ìµœê³ : {best_score:2d} | "
                  f"Epsilon: {agent.epsilon:.3f} | "
                  f"ì‹œê°„: {elapsed_time/60:.1f}ë¶„")
            
            # ì¡°ê¸° ì„±ê³µ ì¡°ê±´
            if recent_avg >= 15:
                print(f"{Colors.GREEN}ğŸ‰ ëª©í‘œ ë‹¬ì„±! í‰ê·  ì ìˆ˜ 15ì  ëŒíŒŒ{Colors.RESET}")
                break
        
        # ì¤‘ê°„ ì €ì¥
        if episode % save_interval == 0 and episode > 0:
            intermediate_name = f"{model_name}_ep{episode}.pkl"
            save_model(agent, scores, avg_scores, intermediate_name, episode)
            print(f"{Colors.BLUE}ğŸ’¾ ì¤‘ê°„ ì €ì¥: {intermediate_name}{Colors.RESET}")
    
    # ìµœì¢… ê²°ê³¼
    final_avg = np.mean(scores[-100:]) if len(scores) >= 100 else np.mean(scores)
    total_time = time.time() - start_time
    
    print(f"\n{Colors.GREEN}ğŸ‰ í›ˆë ¨ ì™„ë£Œ!{Colors.RESET}")
    print(f"{Colors.WHITE}{'='*50}{Colors.RESET}")
    print(f"{Colors.CYAN}ğŸ“Š ìµœì¢… í†µê³„:{Colors.RESET}")
    print(f"   ìµœì¢… í‰ê·  ì ìˆ˜: {final_avg:.2f}")
    print(f"   ìµœê³  ì ìˆ˜: {best_score}")
    print(f"   ìµœê³  í‰ê·  ì ìˆ˜: {best_avg_score:.2f}")
    print(f"   ì´ í›ˆë ¨ ì‹œê°„: {total_time/60:.1f}ë¶„")
    print(f"   Q-í…Œì´ë¸” í¬ê¸°: {len(agent.q_table)}")
    
    # ì„±ëŠ¥ í‰ê°€
    if final_avg >= 15:
        print(f"{Colors.GREEN}ğŸŒŸ íƒì›”í•œ ì„±ëŠ¥! ëª©í‘œ ëŒ€í­ ì´ˆê³¼ ë‹¬ì„±{Colors.RESET}")
    elif final_avg >= 10:
        print(f"{Colors.GREEN}ğŸ¯ ëª©í‘œ ë‹¬ì„±! ìš°ìˆ˜í•œ ì„±ëŠ¥{Colors.RESET}")
    elif final_avg >= 5:
        print(f"{Colors.YELLOW}ğŸ‘ í° ê°œì„ ! ì¶”ê°€ í›ˆë ¨ ê¶Œì¥{Colors.RESET}")
    else:
        print(f"{Colors.RED}ğŸ”„ ì¶”ê°€ ê°œì„  í•„ìš”{Colors.RESET}")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_name = f"{model_name}_final.pkl"
    save_model(agent, scores, avg_scores, final_name, episodes)
    
    # ì‹œê°í™”
    visualize_training_progress(scores, avg_scores)
    
    return {
        'agent': agent,
        'scores': scores,
        'avg_scores': avg_scores,
        'final_avg': final_avg,
        'best_score': best_score,
        'training_time': total_time
    }

def save_model(agent, scores, avg_scores, filename, episodes):
    """ëª¨ë¸ ì €ì¥"""
    model_data = {
        'q_table': agent.q_table,
        'epsilon': agent.epsilon,
        'learning_rate': agent.learning_rate,
        'discount_factor': agent.discount_factor,
        'epsilon_min': agent.epsilon_min,
        'epsilon_decay': agent.epsilon_decay,
        'scores': scores,
        'avg_scores': avg_scores,
        'episodes': episodes,
        'timestamp': time.time()
    }
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ì €ì¥
    save_path = os.path.join(project_root, filename)
    
    with open(save_path, 'wb') as f:
        pickle.dump(model_data, f)
    
    print(f"{Colors.GREEN}âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}{Colors.RESET}")

def compare_with_previous():
    """ê¸°ì¡´ ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ"""
    print(f"\n{Colors.CYAN}ğŸ“Š ê¸°ì¡´ ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ{Colors.RESET}")
    
    # ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ë“¤ ì°¾ê¸°
    old_models = []
    for file in os.listdir(project_root):
        if file.endswith('.pkl') and 'improved' not in file:
            old_models.append(file)
    
    if old_models:
        print(f"{Colors.YELLOW}ë°œê²¬ëœ ê¸°ì¡´ ëª¨ë¸:{Colors.RESET}")
        for model in old_models:
            print(f"   ğŸ“¦ {model}")
        
        # ì²« ë²ˆì§¸ ëª¨ë¸ ë¶„ì„
        try:
            with open(os.path.join(project_root, old_models[0]), 'rb') as f:
                old_data = pickle.load(f)
            
            if 'scores' in old_data:
                old_avg = np.mean(old_data['scores'][-100:]) if len(old_data['scores']) >= 100 else np.mean(old_data['scores'])
                print(f"{Colors.WHITE}ê¸°ì¡´ í‰ê·  ì ìˆ˜: {old_avg:.2f}{Colors.RESET}")
                print(f"{Colors.WHITE}ê°œì„  ëª©í‘œ: {old_avg:.2f} â†’ 10+ (ì•½ {1000/old_avg:.0f}% í–¥ìƒ){Colors.RESET}")
            
        except Exception as e:
            print(f"{Colors.RED}ê¸°ì¡´ ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨: {e}{Colors.RESET}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print(f"{Colors.MAGENTA}ğŸš€ ê°œì„ ëœ Snake AI í›ˆë ¨ ì‹œì‘{Colors.RESET}")
    
    # ê¸°ì¡´ ëª¨ë¸ê³¼ ë¹„êµ
    compare_with_previous()
    
    # ê°œì„ ëœ í›ˆë ¨ ì‹¤í–‰
    result = run_improved_training(
        episodes=3000,      # ì¶©ë¶„í•œ í›ˆë ¨
        save_interval=500,  # ì •ê¸° ì €ì¥
        model_name="snake_improved_agent"
    )
    
    print(f"\n{Colors.CYAN}ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:{Colors.RESET}")
    print(f"{Colors.WHITE}1. demo_runner.pyë¡œ ê°œì„ ëœ ì„±ëŠ¥ í™•ì¸{Colors.RESET}")
    print(f"{Colors.WHITE}2. ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•Šë‹¤ë©´ ë” ê¸´ í›ˆë ¨ (5000+ ì—í”¼ì†Œë“œ){Colors.RESET}")
    print(f"{Colors.WHITE}3. DQN êµ¬í˜„ ê³ ë ¤{Colors.RESET}")

if __name__ == "__main__":
    main()