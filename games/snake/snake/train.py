# ğŸš€ ë±€ê²Œì„ AI - í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (train.py)
# ================================================================
# ì‘ì„±ì: AI/ë¹…ë°ì´í„° ì„ì‚¬ê³¼ì •ìƒ
# í™˜ê²½: VSCode/Colab (ë‹¤í¬ëª¨ë“œ ìµœì í™”)
# ëª©í‘œ: Q-Learningìœ¼ë¡œ ë±€ê²Œì„ì„ ë§ˆìŠ¤í„°í•˜ëŠ” AI í›ˆë ¨
# ================================================================

import numpy as np
import matplotlib.pyplot as plt
import time
import os
from datetime import datetime
from tqdm import tqdm
import json

# ì²´ê³„ì ì¸ êµ¬ì¡°ì—ì„œ íŒ¨í‚¤ì§€ import
from .game import SnakeGameAI
from .agent import QLearningAgent

# ì‹œê°í™”ëŠ” utilsì—ì„œ import
from utils.visualization import TrainingVisualizer

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ import
from utils import create_directory, get_timestamp, save_json, setup_logging

# Colab í™˜ê²½ì—ì„œ ì‹¤í–‰ ì‹œ ì£¼ì„ í•´ì œ
# from IPython.display import clear_output

class SnakeAITrainer:
    """
    ë±€ê²Œì„ AI í•™ìŠµì„ ê´€ë¦¬í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤
    
    ì£¼ìš” ê¸°ëŠ¥:
    - í•™ìŠµ ê³¼ì • ì „ì²´ ê´€ë¦¬
    - ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
    - ëª¨ë¸ ì €ì¥/ë¡œë“œ ê´€ë¦¬
    - ì„±ëŠ¥ í‰ê°€ ë° ì‹œê°í™”
    - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì§€ì›
    
    Attributes:
        game: ë±€ê²Œì„ í™˜ê²½ ì¸ìŠ¤í„´ìŠ¤
        agent: Q-Learning ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤  
        visualizer: í•™ìŠµ ì‹œê°í™” ë„êµ¬
        config: í•™ìŠµ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        training_stats: í•™ìŠµ í†µê³„ ì •ë³´
    """
    
    def __init__(self, config=None):
        """
        AI íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        
        Args:
            config (dict, optional): í•™ìŠµ ì„¤ì • ë”•ì…”ë„ˆë¦¬
            
        ì„¤ëª…:
            - ê²Œì„ í™˜ê²½, ì—ì´ì „íŠ¸, ì‹œê°í™” ë„êµ¬ ì´ˆê¸°í™”
            - ê¸°ë³¸ ì„¤ì •ê°’ ì ìš© ë˜ëŠ” ì‚¬ìš©ì ì„¤ì • ë¡œë“œ
            - ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        """
        # ê¸°ë³¸ ì„¤ì •ê°’
        self.default_config = {
            'episodes': 5000,           # ì´ í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜
            'max_steps': 1000,          # ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í…
            'display_game': True,       # ê²Œì„ í™”ë©´ í‘œì‹œ ì—¬ë¶€
            'display_interval': 100,    # í™”ë©´ í‘œì‹œ ê°„ê²©
            'save_interval': 500,       # ëª¨ë¸ ì €ì¥ ê°„ê²©
            'eval_interval': 100,       # í‰ê°€ ê°„ê²©
            'log_interval': 50,         # ë¡œê·¸ ì¶œë ¥ ê°„ê²©
            'target_score': 20,         # ëª©í‘œ ì ìˆ˜
            'early_stopping': True,     # ì¡°ê¸° ì¢…ë£Œ ì‚¬ìš© ì—¬ë¶€
            'patience': 1000,           # ì¡°ê¸° ì¢…ë£Œ ëŒ€ê¸° ì—í”¼ì†Œë“œ
            
            # Q-Learning í•˜ì´í¼íŒŒë¼ë¯¸í„°
            'learning_rate': 0.1,
            'discount_factor': 0.95,
            'epsilon': 1.0,
            'epsilon_min': 0.01,
            'epsilon_decay': 0.995,
            
            # ê²Œì„ ì„¤ì •
            'game_width': 640,
            'game_height': 480,
            'game_speed': 20
        }
        
        # ì„¤ì • ì ìš©
        self.config = self.default_config.copy()
        if config:
            self.config.update(config)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._initialize_components()
        
        # í•™ìŠµ í†µê³„ ì´ˆê¸°í™”
        self.training_stats = {
            'start_time': None,
            'end_time': None,
            'total_episodes': 0,
            'best_score': 0,
            'best_episode': 0,
            'avg_score_last_100': 0,
            'convergence_episode': None,
            'training_time': 0
        }
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        self.log_dir = f"logs/snake_ai_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(self.log_dir, exist_ok=True)
        os.makedirs("models", exist_ok=True)
        
        print("ğŸš€ ë±€ê²Œì„ AI íŠ¸ë ˆì´ë„ˆê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ ë¡œê·¸ ë””ë ‰í† ë¦¬: {self.log_dir}")
        self._print_config()
    
    def _initialize_components(self):
        """
        ê²Œì„, ì—ì´ì „íŠ¸, ì‹œê°í™” ë„êµ¬ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜
        
        Returns:
            None
            
        ì„¤ëª…:
            - ë±€ê²Œì„ í™˜ê²½ ìƒì„±
            - Q-Learning ì—ì´ì „íŠ¸ ìƒì„±
            - ì‹œê°í™” ë„êµ¬ ì´ˆê¸°í™”
        """
        # ê²Œì„ í™˜ê²½ ì´ˆê¸°í™”
        self.game = SnakeGameAI(
            w=self.config['game_width'],
            h=self.config['game_height'],
            display=self.config['display_game']
        )
        
        # Q-Learning ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        self.agent = QLearningAgent(
            lr=self.config['learning_rate'],
            gamma=self.config['discount_factor'],
            epsilon=self.config['epsilon'],
            epsilon_min=self.config['epsilon_min'],
            epsilon_decay=self.config['epsilon_decay']
        )
        
        # ì‹œê°í™” ë„êµ¬ ì´ˆê¸°í™”
        self.visualizer = TrainingVisualizer()
    
    def _print_config(self):
        """
        í˜„ì¬ ì„¤ì •ì„ ì¶œë ¥í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜
        
        Returns:
            None
        """
        print("\n" + "="*60)
        print("âš™ï¸  í•™ìŠµ ì„¤ì •")
        print("="*60)
        print(f"ğŸ“Š ì´ ì—í”¼ì†Œë“œ: {self.config['episodes']:,}")
        print(f"ğŸ¯ ëª©í‘œ ì ìˆ˜: {self.config['target_score']}")
        print(f"ğŸ§  í•™ìŠµë¥ : {self.config['learning_rate']}")
        print(f"ğŸ² ì´ˆê¸° íƒí—˜ìœ¨: {self.config['epsilon']}")
        print(f"ğŸ’¾ ì €ì¥ ê°„ê²©: {self.config['save_interval']} ì—í”¼ì†Œë“œ")
        print(f"ğŸ“º í™”ë©´ í‘œì‹œ: {'ON' if self.config['display_game'] else 'OFF'}")
        print("="*60)
    
    def train(self, resume_from=None):
        """
        AI í•™ìŠµì„ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
        
        Args:
            resume_from (str, optional): ì¬ê°œí•  ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            dict: í•™ìŠµ ê²°ê³¼ í†µê³„
            
        ì„¤ëª…:
            1. ëª¨ë¸ ë¡œë“œ (ì¬ê°œí•˜ëŠ” ê²½ìš°)
            2. ì—í”¼ì†Œë“œë³„ í•™ìŠµ ë£¨í”„ ì‹¤í–‰
            3. ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
            4. ì£¼ê¸°ì  ëª¨ë¸ ì €ì¥ ë° í‰ê°€
            5. ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ í™•ì¸
            6. ìµœì¢… ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
        """
        print("\nğŸ“ AI í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!")
        
        # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        self.training_stats['start_time'] = datetime.now()
        start_episode = 0
        
        # ëª¨ë¸ ì¬ê°œ (ì„ íƒì‚¬í•­)
        if resume_from and os.path.exists(resume_from):
            if self.agent.load_model(resume_from):
                start_episode = len(self.agent.training_history['scores'])
                print(f"ğŸ”„ {start_episode} ì—í”¼ì†Œë“œë¶€í„° í•™ìŠµì„ ì¬ê°œí•©ë‹ˆë‹¤.")
        
        # ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ ë³€ìˆ˜ë“¤
        best_avg_score = -float('inf')
        no_improvement_count = 0
        
        try:
            # í•™ìŠµ ë£¨í”„
            with tqdm(range(start_episode, self.config['episodes']), 
                     desc="ğŸ§  AI í•™ìŠµ ì¤‘", 
                     ncols=100, 
                     bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:
                
                for episode in pbar:
                    # í•œ ì—í”¼ì†Œë“œ ì‹¤í–‰
                    score, total_reward = self._run_episode(episode)
                    
                    # í•™ìŠµ ê¸°ë¡ ì—…ë°ì´íŠ¸
                    self.agent.update_training_history(score, total_reward, episode)
                    self.training_stats['total_episodes'] = episode + 1
                    
                    # ìµœê³  ì ìˆ˜ ì—…ë°ì´íŠ¸
                    if score > self.training_stats['best_score']:
                        self.training_stats['best_score'] = score
                        self.training_stats['best_episode'] = episode
                    
                    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                    self._update_progress_bar(pbar, episode, score, total_reward)
                    
                    # ì£¼ê¸°ì  ë¡œê·¸ ì¶œë ¥
                    if (episode + 1) % self.config['log_interval'] == 0:
                        self._print_progress(episode)
                    
                    # ì£¼ê¸°ì  ëª¨ë¸ ì €ì¥
                    if (episode + 1) % self.config['save_interval'] == 0:
                        self._save_checkpoint(episode)
                    
                    # ì£¼ê¸°ì  í‰ê°€
                    if (episode + 1) % self.config['eval_interval'] == 0:
                        avg_score = self._evaluate_performance(episode)
                        
                        # ì¡°ê¸° ì¢…ë£Œ í™•ì¸
                        if self.config['early_stopping']:
                            if avg_score > best_avg_score:
                                best_avg_score = avg_score
                                no_improvement_count = 0
                            else:
                                no_improvement_count += self.config['eval_interval']
                            
                            # ëª©í‘œ ë‹¬ì„± ë˜ëŠ” ìˆ˜ë ´ í™•ì¸
                            if (avg_score >= self.config['target_score'] or 
                                no_improvement_count >= self.config['patience']):
                                self._handle_early_stopping(episode, avg_score)
                                break
        
        except KeyboardInterrupt:
            print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        finally:
            # í•™ìŠµ ì¢…ë£Œ ì²˜ë¦¬
            self._finalize_training()
        
        return self.training_stats
    
    def _run_episode(self, episode):
        """
        í•œ ì—í”¼ì†Œë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜
        
        Args:
            episode (int): í˜„ì¬ ì—í”¼ì†Œë“œ ë²ˆí˜¸
            
        Returns:
            tuple: (ìµœì¢… ì ìˆ˜, ì´ ë³´ìƒ)
            
        ì„¤ëª…:
            1. ê²Œì„ í™˜ê²½ ë¦¬ì…‹
            2. ìŠ¤í…ë³„ ê²Œì„ ì§„í–‰
            3. ìƒíƒœ-í–‰ë™-ë³´ìƒ ê¸°ë¡
            4. Q-Learning ì—…ë°ì´íŠ¸
        """
        # ê²Œì„ ì´ˆê¸°í™”
        self.game.reset()
        state = self.game.get_state()
        total_reward = 0
        
        # í™”ë©´ í‘œì‹œ ì—¬ë¶€ ê²°ì •
        display_this_episode = (episode % self.config['display_interval'] == 0)
        
        for step in range(self.config['max_steps']):
            # í–‰ë™ ì„ íƒ
            action = self.agent.get_action(state)
            
            # í–‰ë™ ìˆ˜í–‰
            reward, done, score = self.game.play_step(action)
            next_state = self.game.get_state()
            total_reward += reward
            
            # ê²½í—˜ ì €ì¥
            self.agent.remember(state, action, reward, next_state, done)
            
            # Q-Learning ì—…ë°ì´íŠ¸
            self.agent.learn(state, action, reward, next_state, done)
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state = next_state
            
            # ê²Œì„ ì¢…ë£Œ í™•ì¸
            if done:
                break
            
            # í™”ë©´ í‘œì‹œ ì†ë„ ì¡°ì ˆ
            if display_this_episode and self.config['display_game']:
                time.sleep(0.05)  # ê´€ì°°í•˜ê¸° ì‰½ë„ë¡ ì†ë„ ì¡°ì ˆ
        
        return score, total_reward
    
    def _update_progress_bar(self, pbar, episode, score, total_reward):
        """
        ì§„í–‰ë¥  í‘œì‹œì¤„ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜
        
        Args:
            pbar: tqdm ì§„í–‰ë¥  í‘œì‹œì¤„ ê°ì²´
            episode (int): í˜„ì¬ ì—í”¼ì†Œë“œ
            score (int): ì—í”¼ì†Œë“œ ì ìˆ˜
            total_reward (float): ì—í”¼ì†Œë“œ ì´ ë³´ìƒ
            
        Returns:
            None
        """
        # ìµœê·¼ 100 ì—í”¼ì†Œë“œ í‰ê·  ê³„ì‚°
        recent_scores = self.agent.training_history['scores'][-100:]
        avg_score = np.mean(recent_scores) if recent_scores else 0
        
        # ì§„í–‰ë¥  í‘œì‹œì¤„ ì—…ë°ì´íŠ¸
        pbar.set_postfix({
            'ì ìˆ˜': f'{score:3d}',
            'í‰ê· ': f'{avg_score:5.1f}',
            'ìµœê³ ': f'{self.training_stats["best_score"]:3d}',
            'Îµ': f'{self.agent.epsilon:.3f}'
        })
    
    def _print_progress(self, episode):
        """
        ìƒì„¸í•œ ì§„í–‰ ìƒí™©ì„ ì¶œë ¥í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜
        
        Args:
            episode (int): í˜„ì¬ ì—í”¼ì†Œë“œ ë²ˆí˜¸
            
        Returns:
            None
        """
        recent_scores = self.agent.training_history['scores'][-self.config['log_interval']:]
        recent_rewards = self.agent.training_history['rewards'][-self.config['log_interval']:]
        
        if recent_scores:
            avg_score = np.mean(recent_scores)
            avg_reward = np.mean(recent_rewards)
            q_stats = self.agent.get_q_table_stats()
            
            print(f"\nğŸ“Š ì—í”¼ì†Œë“œ {episode+1:,}")
            print(f"   ì ìˆ˜: {recent_scores[-1]:3d} | í‰ê· : {avg_score:5.1f} | ìµœê³ : {self.training_stats['best_score']:3d}")
            print(f"   ë³´ìƒ: {avg_reward:7.2f} | Epsilon: {self.agent.epsilon:.4f}")
            print(f"   Q-í…Œì´ë¸”: {q_stats['size']:,}ê°œ ìƒíƒœ | í‰ê·  Qê°’: {q_stats['avg_q_value']:.3f}")
    
    def _evaluate_performance(self, episode):
        """
        í˜„ì¬ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜
        
        Args:
            episode (int): í˜„ì¬ ì—í”¼ì†Œë“œ ë²ˆí˜¸
            
        Returns:
            float: ìµœê·¼ 100 ì—í”¼ì†Œë“œ í‰ê·  ì ìˆ˜
        """
        recent_scores = self.agent.training_history['scores'][-100:]
        avg_score = np.mean(recent_scores) if len(recent_scores) >= 50 else 0
        
        self.training_stats['avg_score_last_100'] = avg_score
        
        print(f"\nğŸ¯ ì„±ëŠ¥ í‰ê°€ (ì—í”¼ì†Œë“œ {episode+1})")
        print(f"   ìµœê·¼ 100 ì—í”¼ì†Œë“œ í‰ê· : {avg_score:.2f}")
        print(f"   ëª©í‘œ ì ìˆ˜: {self.config['target_score']}")
        print(f"   ë‹¬ì„±ë¥ : {(avg_score / self.config['target_score'] * 100):5.1f}%")
        
        return avg_score
    
    def _handle_early_stopping(self, episode, avg_score):
        """
        ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ì„ ì²˜ë¦¬í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜
        
        Args:
            episode (int): í˜„ì¬ ì—í”¼ì†Œë“œ ë²ˆí˜¸
            avg_score (float): í‰ê·  ì ìˆ˜
            
        Returns:
            None
        """
        self.training_stats['convergence_episode'] = episode + 1
        
        if avg_score >= self.config['target_score']:
            print(f"\nğŸ‰ ëª©í‘œ ë‹¬ì„±! ì—í”¼ì†Œë“œ {episode+1}ì—ì„œ í‰ê·  ì ìˆ˜ {avg_score:.2f} ë‹¬ì„±!")
        else:
            print(f"\nâ¹ï¸ í•™ìŠµ ìˆ˜ë ´! ì—í”¼ì†Œë“œ {episode+1}ì—ì„œ ì¡°ê¸° ì¢…ë£Œ")
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        final_model_path = f"models/snake_ai_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
        self.agent.save_model(final_model_path)
    
    def _save_checkpoint(self, episode):
        """
        ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜
        
        Args:
            episode (int): í˜„ì¬ ì—í”¼ì†Œë“œ ë²ˆí˜¸
            
        Returns:
            None
        """
        checkpoint_path = f"models/snake_ai_checkpoint_ep{episode+1}.pkl"
        self.agent.save_model(checkpoint_path)
        
        # ì„¤ì • íŒŒì¼ë„ í•¨ê»˜ ì €ì¥
        config_path = f"{self.log_dir}/config.json"
        with open(config_path, 'w') as f:
            json.dump(self.config, f, indent=2)
    
    def _finalize_training(self):
        """
        í•™ìŠµ ì¢…ë£Œ í›„ ì •ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜
        
        Returns:
            None
        """
        # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
        self.training_stats['end_time'] = datetime.now()
        self.training_stats['training_time'] = (
            self.training_stats['end_time'] - self.training_stats['start_time']
        ).total_seconds() / 60  # ë¶„ ë‹¨ìœ„
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        if not self.training_stats['convergence_episode']:
            final_model_path = f"models/snake_ai_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
            self.agent.save_model(final_model_path)
        
        # í•™ìŠµ í†µê³„ ì €ì¥
        stats_path = f"{self.log_dir}/training_stats.json"
        with open(stats_path, 'w') as f:
            # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            stats_to_save = self.training_stats.copy()
            if stats_to_save['start_time']:
                stats_to_save['start_time'] = stats_to_save['start_time'].isoformat()
            if stats_to_save['end_time']:
                stats_to_save['end_time'] = stats_to_save['end_time'].isoformat()
            json.dump(stats_to_save, f, indent=2)
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        self._print_final_results()
        
        # í•™ìŠµ ê·¸ë˜í”„ ìƒì„±
        graph_path = f"{self.log_dir}/training_progress.png"
        self.visualizer.plot_training_progress(self.agent, save_path=graph_path)
    
    def _print_final_results(self):
        """
        ìµœì¢… í•™ìŠµ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜
        
        Returns:
            None
        """
        print("\n" + "="*60)
        print("ğŸ† í•™ìŠµ ì™„ë£Œ!")
        print("="*60)
        print(f"â±ï¸  ì´ í•™ìŠµ ì‹œê°„: {self.training_stats['training_time']:.1f}ë¶„")
        print(f"ğŸ“Š ì´ ì—í”¼ì†Œë“œ: {self.training_stats['total_episodes']:,}")
        print(f"ğŸ¥‡ ìµœê³  ì ìˆ˜: {self.training_stats['best_score']} (ì—í”¼ì†Œë“œ {self.training_stats['best_episode']+1})")
        print(f"ğŸ“ˆ ìµœì¢… í‰ê·  ì ìˆ˜: {self.training_stats['avg_score_last_100']:.2f}")
        
        if self.training_stats['convergence_episode']:
            print(f"ğŸ¯ ìˆ˜ë ´ ì—í”¼ì†Œë“œ: {self.training_stats['convergence_episode']}")
        
        q_stats = self.agent.get_q_table_stats()
        print(f"ğŸ§  Q-í…Œì´ë¸” í¬ê¸°: {q_stats['size']:,}ê°œ ìƒíƒœ")
        print(f"ğŸ“ ë¡œê·¸ ë””ë ‰í† ë¦¬: {self.log_dir}")
        print("="*60)
    
    def test_trained_agent(self, model_path, test_episodes=10):
        """
        í•™ìŠµëœ ì—ì´ì „íŠ¸ë¥¼ í…ŒìŠ¤íŠ¸í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            model_path (str): í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            test_episodes (int): í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ìˆ˜
            
        Returns:
            dict: í…ŒìŠ¤íŠ¸ ê²°ê³¼ í†µê³„
        """
        print(f"\nğŸ§ª í•™ìŠµëœ AI í…ŒìŠ¤íŠ¸ (ëª¨ë¸: {model_path})")
        
        # ëª¨ë¸ ë¡œë“œ
        if not self.agent.load_model(model_path):
            print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            return None
        
        # íƒí—˜ ë¹„í™œì„±í™” (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)
        original_epsilon = self.agent.epsilon
        self.agent.epsilon = 0.0  # ì™„ì „íˆ í•™ìŠµëœ ì •ì±… ì‚¬ìš©
        
        test_scores = []
        test_rewards = []
        
        print(f"ğŸ® {test_episodes}ë²ˆì˜ í…ŒìŠ¤íŠ¸ ê²Œì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤...")
        
        for episode in range(test_episodes):
            # ê²Œì„ ì‹¤í–‰
            self.game.reset()
            state = self.game.get_state()
            total_reward = 0
            
            for step in range(self.config['max_steps']):
                action, q_values = self.agent.get_best_action_for_state(state)
                reward, done, score = self.game.play_step(action)
                state = self.game.get_state()
                total_reward += reward
                
                if done:
                    break
            
            test_scores.append(score)
            test_rewards.append(total_reward)
            
            print(f"  ê²Œì„ {episode+1}: ì ìˆ˜ {score}, ë³´ìƒ {total_reward:.1f}")
        
        # ì›ë˜ epsilon ë³µì›
        self.agent.epsilon = original_epsilon
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
        test_results = {
            'avg_score': np.mean(test_scores),
            'max_score': np.max(test_scores),
            'min_score': np.min(test_scores),
            'std_score': np.std(test_scores),
            'avg_reward': np.mean(test_rewards),
            'scores': test_scores,
            'rewards': test_rewards
        }
        
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"   í‰ê·  ì ìˆ˜: {test_results['avg_score']:.2f} Â± {test_results['std_score']:.2f}")
        print(f"   ìµœê³  ì ìˆ˜: {test_results['max_score']}")
        print(f"   ìµœì € ì ìˆ˜: {test_results['min_score']}")
        print(f"   í‰ê·  ë³´ìƒ: {test_results['avg_reward']:.2f}")
        
        return test_results

# ================================================================
# ì‹¤í–‰ í•¨ìˆ˜ë“¤
# ================================================================

def main_training():
    """
    ë©”ì¸ í•™ìŠµ ì‹¤í–‰ í•¨ìˆ˜
    
    Returns:
        None
        
    ì„¤ëª…:
        - ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ AI í•™ìŠµ ì‹œì‘
        - ì‚¬ìš©ìê°€ ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ë˜í¼ í•¨ìˆ˜
    """
    print("ğŸ® ë±€ê²Œì„ AI í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    
    # ê¸°ë³¸ ì„¤ì •
    config = {
        'episodes': 3000,
        'target_score': 15,
        'display_game': True,
        'display_interval': 50,
        'save_interval': 300,
        'learning_rate': 0.1,
        'epsilon_decay': 0.996
    }
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í•™ìŠµ ì‹œì‘
    trainer = SnakeAITrainer(config)
    results = trainer.train()
    
    print("ğŸ‰ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    return results

def quick_demo():
    """
    ë¹ ë¥¸ ë°ëª¨ë¥¼ ìœ„í•œ í•¨ìˆ˜ (ì§§ì€ í•™ìŠµ)
    
    Returns:
        None
    """
    print("âš¡ ë¹ ë¥¸ ë°ëª¨ ëª¨ë“œ (500 ì—í”¼ì†Œë“œ)")
    
    config = {
        'episodes': 500,
        'target_score': 10,
        'display_game': True,
        'display_interval': 25,
        'save_interval': 100,
        'log_interval': 25
    }
    
    trainer = SnakeAITrainer(config)
    results = trainer.train()
    
    return results

def advanced_training():
    """
    ê³ ê¸‰ ì„¤ì •ìœ¼ë¡œ ì¥ì‹œê°„ í•™ìŠµí•˜ëŠ” í•¨ìˆ˜
    
    Returns:
        None
    """
    print("ğŸš€ ê³ ê¸‰ ëª¨ë“œ: ì¥ì‹œê°„ ì •ë°€ í•™ìŠµ")
    
    config = {
        'episodes': 10000,
        'target_score': 25,
        'display_game': False,  # í•™ìŠµ ì†ë„ í–¥ìƒ
        'display_interval': 200,
        'save_interval': 500,
        'learning_rate': 0.05,  # ë” ì•ˆì •ì ì¸ í•™ìŠµ
        'epsilon_decay': 0.9995,  # ë” ì²œì²œíˆ íƒí—˜ ê°ì†Œ
        'patience': 2000  # ë” ì˜¤ë˜ ê¸°ë‹¤ë¦¼
    }
    
    trainer = SnakeAITrainer(config)
    results = trainer.train()
    
    return results

if __name__ == "__main__":
    print("ğŸ ë±€ê²Œì„ AI ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("\nì‹¤í–‰ ì˜µì…˜:")
    print("1. main_training() - ê¸°ë³¸ í•™ìŠµ (3000 ì—í”¼ì†Œë“œ)")
    print("2. quick_demo() - ë¹ ë¥¸ ë°ëª¨ (500 ì—í”¼ì†Œë“œ)")  
    print("3. advanced_training() - ê³ ê¸‰ í•™ìŠµ (10000 ì—í”¼ì†Œë“œ)")
    print("\nì˜ˆì‹œ:")
    print(">>> results = main_training()")
    
    results = main_training()