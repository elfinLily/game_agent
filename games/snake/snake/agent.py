# ğŸ§  ë±€ê²Œì„ AI - Q-Learning ì—ì´ì „íŠ¸ êµ¬í˜„

import numpy as np
import random
import pickle
import os
from collections import defaultdict, deque
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json

# ë‹¤í¬ëª¨ë“œ ì‹œê°í™” ì„¤ì •
plt.style.use('dark_background')

class QLearningAgent:
    """
    Q-Learning ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ í´ë˜ìŠ¤
    
    Q-Learning í•µì‹¬ ì›ë¦¬:
    - Q(s,a) = Q(s,a) + Î±[r + Î³*max(Q(s',a')) - Q(s,a)]
    - s: í˜„ì¬ ìƒíƒœ, a: í˜„ì¬ í–‰ë™, r: ë³´ìƒ
    - s': ë‹¤ìŒ ìƒíƒœ, Î±: í•™ìŠµë¥ , Î³: í• ì¸ì¸ìˆ˜
    
    Attributes:
        lr (float): í•™ìŠµë¥  (Learning Rate)
        gamma (float): í• ì¸ì¸ìˆ˜ (Discount Factor)  
        epsilon (float): íƒí—˜ í™•ë¥  (Exploration Rate)
        epsilon_min (float): ìµœì†Œ íƒí—˜ í™•ë¥ 
        epsilon_decay (float): íƒí—˜ í™•ë¥  ê°ì†Œìœ¨
        q_table (defaultdict): Q-í…Œì´ë¸” (ìƒíƒœ-í–‰ë™ ìŒì˜ Qê°’ ì €ì¥)
        memory (deque): ìµœê·¼ ê²½í—˜ ì €ì¥ì†Œ
        training_history (dict): í•™ìŠµ ì§„í–‰ ê¸°ë¡
    """
    
    def __init__(self, lr=0.1, gamma=0.95, epsilon=1.0, 
                 epsilon_min=0.01, epsilon_decay=0.995):
        """
        Q-Learning ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Args:
            lr (float): í•™ìŠµë¥  - Qê°’ ì—…ë°ì´íŠ¸ ì†ë„ ì¡°ì ˆ (ê¸°ë³¸ê°’: 0.1)
            gamma (float): í• ì¸ì¸ìˆ˜ - ë¯¸ë˜ ë³´ìƒ ì¤‘ìš”ë„ (ê¸°ë³¸ê°’: 0.95)
            epsilon (float): ì´ˆê¸° íƒí—˜ í™•ë¥  (ê¸°ë³¸ê°’: 1.0 = 100% íƒí—˜)
            epsilon_min (float): ìµœì†Œ íƒí—˜ í™•ë¥  (ê¸°ë³¸ê°’: 0.01 = 1% íƒí—˜)
            epsilon_decay (float): íƒí—˜ í™•ë¥  ê°ì†Œìœ¨ (ê¸°ë³¸ê°’: 0.995)
            
        ì„¤ëª…:
            - ë†’ì€ í•™ìŠµë¥ : ë¹ ë¥¸ í•™ìŠµ, í•˜ì§€ë§Œ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ
            - ë†’ì€ í• ì¸ì¸ìˆ˜: ì¥ê¸°ì  ë³´ìƒ ì¤‘ì‹œ
            - ë†’ì€ íƒí—˜ í™•ë¥ : ì´ˆê¸°ì—” ë§ì´ íƒí—˜, ì ì§„ì ìœ¼ë¡œ ê°ì†Œ
        """
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
        self.lr = lr
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        
        # Q-í…Œì´ë¸” ì´ˆê¸°í™” (defaultdictë¡œ ìë™ 0 ì´ˆê¸°í™”)
        self.q_table = {}
        
        # ë©”ëª¨ë¦¬ ë° ê¸°ë¡ ì´ˆê¸°í™”
        self.memory = deque(maxlen=10000)  # ìµœê·¼ 10000ê°œ ê²½í—˜ë§Œ ì €ì¥
        self.training_history = {
            'scores': [],
            'rewards': [],
            'epsilons': [],
            'q_table_size': [],
            'avg_q_values': []
        }
        
        print("ğŸ§  Q-Learning ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„°: lr={lr}, gamma={gamma}, epsilon={epsilon}")
    
    def get_state_key(self, state):
        """
        ìƒíƒœ ë°°ì—´ì„ Q-í…Œì´ë¸”ì˜ í‚¤ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            state (np.array): ê²Œì„ ìƒíƒœ ë²¡í„° (í¬ê¸° 11)
            
        Returns:
            str: Q-í…Œì´ë¸”ì—ì„œ ì‚¬ìš©í•  ë¬¸ìì—´ í‚¤
            
        ì„¤ëª…:
            - NumPy ë°°ì—´ì€ ë”•ì…”ë„ˆë¦¬ í‚¤ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ
            - ë°°ì—´ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ í‚¤ë¡œ ì‚¬ìš©
            - ì˜ˆ: [1,0,0,1,0,0,0,1,0,0,0] -> "1,0,0,1,0,0,0,1,0,0,0"
        """
        return ','.join(map(str, state.astype(int)))
    
    def get_action(self, state):
        """
        í˜„ì¬ ìƒíƒœì—ì„œ í–‰ë™ì„ ì„ íƒí•˜ëŠ” í•¨ìˆ˜ (Epsilon-Greedy ì •ì±…)
        
        Args:
            state (np.array): í˜„ì¬ ê²Œì„ ìƒíƒœ
            
        Returns:
            np.array: ì„ íƒëœ í–‰ë™ ë²¡í„° [ì§ì§„, ìš°íšŒì „, ì¢ŒíšŒì „]
            
        ì„¤ëª…:
            Epsilon-Greedy ì •ì±…:
            - epsilon í™•ë¥ ë¡œ ëœë¤ í–‰ë™ (íƒí—˜, Exploration)
            - (1-epsilon) í™•ë¥ ë¡œ ìµœì  í–‰ë™ (í™œìš©, Exploitation)
            - í•™ìŠµ ì´ˆê¸°: ë†’ì€ íƒí—˜, í•™ìŠµ í›„ê¸°: ë†’ì€ í™œìš©
        """
        state_key = self.get_state_key(state)
        
        # Epsilon-Greedy ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ
        if random.random() < self.epsilon:
            # íƒí—˜: ëœë¤ í–‰ë™ ì„ íƒ
            action_idx = random.randint(0, 2)
        else:
            # í™œìš©: Qê°’ì´ ê°€ì¥ ë†’ì€ í–‰ë™ ì„ íƒ
            q_values = self.q_table[state_key]
            action_idx = np.argmax(q_values)
        
        # í–‰ë™ì„ ì›-í•« ë²¡í„°ë¡œ ë³€í™˜
        action = np.zeros(3)
        action[action_idx] = 1
        
        return action
    
    def remember(self, state, action, reward, next_state, done):
        """
        ê²½í—˜ì„ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            state (np.array): í˜„ì¬ ìƒíƒœ
            action (np.array): ìˆ˜í–‰í•œ í–‰ë™
            reward (float): ë°›ì€ ë³´ìƒ
            next_state (np.array): ë‹¤ìŒ ìƒíƒœ  
            done (bool): ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€
            
        Returns:
            None
            
        ì„¤ëª…:
            - ê°•í™”í•™ìŠµì˜ í•µì‹¬ì¸ (s, a, r, s', done) íŠœí”Œ ì €ì¥
            - ë‚˜ì¤‘ì— Qê°’ ì—…ë°ì´íŠ¸ì— ì‚¬ìš©
            - deque ìë£Œêµ¬ì¡°ë¡œ ìµœì‹  ê²½í—˜ë§Œ ìœ ì§€
        """
        self.memory.append((state, action, reward, next_state, done))
    
    def learn(self, state, action, reward, next_state, done):
        """
        Q-Learning ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ Qê°’ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            state (np.array): í˜„ì¬ ìƒíƒœ
            action (np.array): ìˆ˜í–‰í•œ í–‰ë™
            reward (float): ë°›ì€ ë³´ìƒ
            next_state (np.array): ë‹¤ìŒ ìƒíƒœ
            done (bool): ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€
            
        Returns:
            None
            
        ì„¤ëª…:
            Q-Learning ì—…ë°ì´íŠ¸ ê³µì‹:
            Q(s,a) = Q(s,a) + Î±[r + Î³*max(Q(s',a')) - Q(s,a)]
            
            - í˜„ì¬ Qê°’ê³¼ ëª©í‘œê°’ì˜ ì°¨ì´ë§Œí¼ ì—…ë°ì´íŠ¸
            - ëª©í‘œê°’ = ì¦‰ì‹œë³´ìƒ + í• ì¸ëœ ë¯¸ë˜ ìµœëŒ€ Qê°’
            - ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ë¯¸ë˜ ë³´ìƒì€ 0
        """
        # ìƒíƒœì™€ í–‰ë™ì„ í‚¤ë¡œ ë³€í™˜
        state_key = self.get_state_key(state)
        next_state_key = self.get_state_key(next_state)
        action_idx = np.argmax(action)
        
        # í˜„ì¬ Qê°’
        current_q = self.q_table[state_key][action_idx]
        
        # ëª©í‘œ Qê°’ ê³„ì‚°
        if done:
            # ê²Œì„ ì¢…ë£Œ ì‹œ ë¯¸ë˜ ë³´ìƒ ì—†ìŒ
            target_q = reward
        else:
            # ë‹¤ìŒ ìƒíƒœì—ì„œ ìµœëŒ€ Qê°’
            max_next_q = np.max(self.q_table[next_state_key])
            target_q = reward + self.gamma * max_next_q
        
        # Qê°’ ì—…ë°ì´íŠ¸ (Q-Learning ê³µì‹)
        self.q_table[state_key][action_idx] += self.lr * (target_q - current_q)
        
        # íƒí—˜ í™•ë¥  ê°ì†Œ (ì ì§„ì ìœ¼ë¡œ í™œìš© ìœ„ì£¼ë¡œ ë³€ê²½)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def batch_learn(self, batch_size=32):
        """
        ë©”ëª¨ë¦¬ì—ì„œ ë°°ì¹˜ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ í•œ ë²ˆì— í•™ìŠµí•˜ëŠ” í•¨ìˆ˜
        
        Args:
            batch_size (int): ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 32)
            
        Returns:
            None
            
        ì„¤ëª…:
            - ë©”ëª¨ë¦¬ì— ì¶©ë¶„í•œ ê²½í—˜ì´ ìŒ“ì´ë©´ ë°°ì¹˜ í•™ìŠµ ìˆ˜í–‰
            - ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ë°ì´í„° ìƒê´€ê´€ê³„ ì œê±°
            - í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
        """
        if len(self.memory) < batch_size:
            return
        
        # ëœë¤í•˜ê²Œ ë°°ì¹˜ ìƒ˜í”Œë§
        batch = random.sample(self.memory, batch_size)
        
        # ë°°ì¹˜ì˜ ê° ê²½í—˜ì— ëŒ€í•´ í•™ìŠµ
        for state, action, reward, next_state, done in batch:
            self.learn(state, action, reward, next_state, done)
    
    def get_q_table_stats(self):
        """
        Q-í…Œì´ë¸”ì˜ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
        
        Returns:
            dict: Q-í…Œì´ë¸” í†µê³„ ì •ë³´
            
        ë°˜í™˜ ì •ë³´:
            - size: Q-í…Œì´ë¸” í¬ê¸° (ìƒíƒœ ìˆ˜)
            - avg_q_value: í‰ê·  Qê°’
            - max_q_value: ìµœëŒ€ Qê°’
            - min_q_value: ìµœì†Œ Qê°’
            - explored_states: íƒí—˜í•œ ìƒíƒœ ìˆ˜
        """
        if not self.q_table:
            return {
                'size': 0,
                'avg_q_value': 0,
                'max_q_value': 0,
                'min_q_value': 0,
                'explored_states': 0
            }
        
        all_q_values = []
        for q_values in self.q_table.values():
            all_q_values.extend(q_values)
        
        all_q_values = np.array(all_q_values)
        
        return {
            'size': len(self.q_table),
            'avg_q_value': np.mean(all_q_values),
            'max_q_value': np.max(all_q_values),
            'min_q_value': np.min(all_q_values),
            'explored_states': len(self.q_table)
        }
    
    def save_model(self, filepath):
        """
        í•™ìŠµëœ Q-í…Œì´ë¸”ê³¼ ì—ì´ì „íŠ¸ ìƒíƒœë¥¼ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            filepath (str): ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
            
        Returns:
            None
            
        ì„¤ëª…:
            - Q-í…Œì´ë¸”, í•˜ì´í¼íŒŒë¼ë¯¸í„°, í•™ìŠµ ê¸°ë¡ì„ ëª¨ë‘ ì €ì¥
            - pickle í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ì—¬ ë‚˜ì¤‘ì— ë¡œë“œ ê°€ëŠ¥
            - ëª¨ë¸ ë²„ì „ê³¼ ì €ì¥ ì‹œê°„ë„ í•¨ê»˜ ê¸°ë¡
        """
        model_data = {
            'q_table': dict(self.q_table),  # defaultdictì„ ì¼ë°˜ dictë¡œ ë³€í™˜
            'lr': self.lr,
            'gamma': self.gamma,
            'epsilon': self.epsilon,
            'epsilon_min': self.epsilon_min,
            'epsilon_decay': self.epsilon_decay,
            'training_history': self.training_history,
            'save_time': datetime.now().isoformat(),
            'version': '1.0'
        }
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"ğŸ’¾ ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
        print(f"ğŸ“Š Q-í…Œì´ë¸” í¬ê¸°: {len(self.q_table)}ê°œ ìƒíƒœ")
    
    def load_model(self, filepath):
        """
        ì €ì¥ëœ Q-í…Œì´ë¸”ê³¼ ì—ì´ì „íŠ¸ ìƒíƒœë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
        
        Args:
            filepath (str): ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ
            
        Returns:
            bool: ë¡œë“œ ì„±ê³µ ì—¬ë¶€
            
        ì„¤ëª…:
            - ì´ì „ì— ì €ì¥ëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ í•™ìŠµ ì¬ê°œ ê°€ëŠ¥
            - ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ í•™ìŠµ ê¸°ë¡ë„ ë³µì›
        """
        try:
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            
            # Q-í…Œì´ë¸” ë³µì›
            self.q_table = defaultdict(lambda: np.zeros(3))
            self.q_table.update(model_data['q_table'])
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³µì›
            self.lr = model_data['lr']
            self.gamma = model_data['gamma']
            self.epsilon = model_data['epsilon']
            self.epsilon_min = model_data['epsilon_min']
            self.epsilon_decay = model_data['epsilon_decay']
            
            # í•™ìŠµ ê¸°ë¡ ë³µì›
            self.training_history = model_data['training_history']
            
            print(f"ğŸ“‚ ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
            print(f"ğŸ“Š Q-í…Œì´ë¸” í¬ê¸°: {len(self.q_table)}ê°œ ìƒíƒœ")
            print(f"ğŸ¯ í˜„ì¬ Epsilon: {self.epsilon:.4f}")
            
            return True
            
        except FileNotFoundError:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
            return False
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def update_training_history(self, score, total_reward, episode):
        """
        í•™ìŠµ ì§„í–‰ ìƒí™©ì„ ê¸°ë¡í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            score (int): ì—í”¼ì†Œë“œ ì ìˆ˜
            total_reward (float): ì—í”¼ì†Œë“œ ì´ ë³´ìƒ
            episode (int): ì—í”¼ì†Œë“œ ë²ˆí˜¸
            
        Returns:
            None
            
        ì„¤ëª…:
            - ì—í”¼ì†Œë“œë³„ ì„±ê³¼ë¥¼ ê¸°ë¡í•˜ì—¬ í•™ìŠµ ë¶„ì„ì— í™œìš©
            - ë‚˜ì¤‘ì— ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•  ë°ì´í„° ì¶•ì 
        """
        self.training_history['scores'].append(score)
        self.training_history['rewards'].append(total_reward)
        self.training_history['epsilons'].append(self.epsilon)
        
        # Q-í…Œì´ë¸” í†µê³„ ê¸°ë¡
        stats = self.get_q_table_stats()
        self.training_history['q_table_size'].append(stats['size'])
        self.training_history['avg_q_values'].append(stats['avg_q_value'])
    
    def get_best_action_for_state(self, state):
        """
        ì£¼ì–´ì§„ ìƒíƒœì—ì„œ ìµœì  í–‰ë™ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ (íƒí—˜ ì—†ì´)
        
        Args:
            state (np.array): ê²Œì„ ìƒíƒœ
            
        Returns:
            tuple: (ìµœì  í–‰ë™ ë²¡í„°, Qê°’ ë°°ì—´)
            
        ì„¤ëª…:
            - í…ŒìŠ¤íŠ¸ë‚˜ ì‹¤ì œ í”Œë ˆì´ ì‹œ ì‚¬ìš©
            - íƒí—˜ ì—†ì´ ìˆœìˆ˜í•˜ê²Œ í•™ìŠµëœ ì •ì±…ë§Œ ì‚¬ìš©
        """
        state_key = self.get_state_key(state)
        q_values = self.q_table[state_key]
        
        # ìµœì  í–‰ë™ ì„ íƒ
        action_idx = np.argmax(q_values)
        action = np.zeros(3)
        action[action_idx] = 1
        
        return action, q_values

# ================================================================
# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# ================================================================

def test_agent():
    """
    Q-Learning ì—ì´ì „íŠ¸ ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    
    Returns:
        None
        
    ì„¤ëª…:
        - ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        - ê¸°ë³¸ ë©”ì„œë“œë“¤ ë™ì‘ í™•ì¸
        - ê°„ë‹¨í•œ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜
    """
    print("ğŸ§ª Q-Learning ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*50)
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = QLearningAgent()
    
    # ê°€ìƒì˜ ìƒíƒœì™€ í–‰ë™ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    test_state = np.array([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1])
    
    print(f"ğŸ” í…ŒìŠ¤íŠ¸ ìƒíƒœ: {test_state}")
    
    # í–‰ë™ ì„ íƒ í…ŒìŠ¤íŠ¸
    action = agent.get_action(test_state)
    print(f"ğŸ¯ ì„ íƒëœ í–‰ë™: {action}")
    
    # í•™ìŠµ í…ŒìŠ¤íŠ¸
    next_state = np.array([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1])
    reward = 5.0
    done = False
    
    agent.learn(test_state, action, reward, next_state, done)
    print(f"ğŸ“š í•™ìŠµ ì™„ë£Œ - ë³´ìƒ: {reward}")
    
    # Q-í…Œì´ë¸” í†µê³„
    stats = agent.get_q_table_stats()
    print(f"ğŸ“Š Q-í…Œì´ë¸” í†µê³„: {stats}")
    
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    print("ğŸ§  Q-Learning ì—ì´ì „íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("\ní…ŒìŠ¤íŠ¸ ì‹¤í–‰:")
    test_agent()